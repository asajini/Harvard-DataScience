---
title: "2 - Linear Models"
author: "Sajini Arumugam"
output: html_document
weight : 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Question 1

Why is the number of home runs considered a confounder of the relationship between bases on balls and runs per game?
- Home runs is not a confounder of this relationship.
- Home runs are the primary cause of runs per game.
- The correlation between home runs and runs per game is stronger than the correlation between bases on balls and runs per game.
- Players who get more bases on balls also tend to have more home runs; in addition, home runs increase the points per game.    [*]


Question 2

As described in the videos, when we stratified our regression lines for runs per game vs. bases on balls by the number of home runs, what happened?
- The slope of runs per game vs. bases on balls within each stratum was reduced because we removed confounding by home runs.   [*]
- The slope of runs per game vs. bases on balls within each stratum was reduced because there were fewer data points.
- The slope of runs per game vs. bases on balls within each stratum increased after we removed confounding by home runs.
- The slope of runs per game vs. bases on balls within each stratum stayed about the same as the original slope.


Question 3

We run a linear model for sons’ heights vs. fathers’ heights using the Galton height data, and get the following results:

```
 > lm(son ~ father, data = galton_heights)

Call:
lm(formula = son ~ father, data = galton_heights)

Coefficients:
(Intercept)    father  
    35.71       0.50  
```

Interpret the numeric coefficient for "father."
- For every inch we increase the son’s height, the predicted father’s height increases by 0.5 inches.
- For every inch we increase the father’s height, the predicted son’s height grows by 0.5 inches.   [*]
- For every inch we increase the father’s height, the predicted son’s height is 0.5 times greater.

Question 4

We want the intercept term for our model to be more interpretable, so we run the same model as before but now we subtract the mean of fathers’ heights from each individual father’s height to create a new variable centered at zero.

```
galton_heights <- galton_heights %>%
    mutate(father_centered=father - mean(father))
```

We run a linear model using this centered fathers’ height variable.


> lm(son ~ father_centered, data = galton_heights)

Call:
lm(formula = son ~ father_centered, data = galton_heights)

Coefficients:
(Intercept)    father_centered  
    70.45          0.50  
    
Interpret the numeric coefficient for the intercept.
- The height of a son of a father of average height is 70.45 inches.   [*]
- The height of a son when a father’s height is zero is 70.45 inches.
- The height of an average father is 70.45 inches.


Question 5

Suppose we fit a multivariate regression model for expected runs based on BB and HR:

E[R|BB=x1,HR=x2]=β0+β1x1+β2x2 
Suppose we fix  BB=x1 . Then we observe a linear relationship between runs and HR with intercept of:

- β0 
- β0+β2x2 
- β0+β1x1 [*]
- β0+β2x1Suppose we fix  BB=x1 

Question 6

Which of the following are assumptions for the errors ϵi in a linear regression model?

- The  ϵi  are independent of each other[*]
- The  ϵi  have expected value 0[*]
- The variance of  ϵi  is a constant[*]


***

Assessment: Least Squares Estimates, part 1


Question 1

The following code was used in the video to plot RSS with  β0=25 .

```
beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss), col=2)
```

In a model for sons’ heights vs fathers’ heights, what is the least squares estimate (LSE) for  β1  if we assume  β^0  is 36?

- 0.65
- 0.5   [*]
- 0.2
- 12