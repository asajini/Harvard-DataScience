---
title: '7.1 Final Assessment: Breast Cancer Prediction Project'
author: "Sajini Arumugam"
output: html_document
weight: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Breast Cancer Project Part 1

The brca dataset from the dslabs package contains information about breast cancer diagnosis biopsy samples for tumors that were determined to be either benign (not cancer) and malignant (cancer). The brca object is a list consisting of:

brca$y: a vector of sample classifications ("B" = benign or "M" = malignant)
brca$x: a matrix of numeric features describing properties of the shape and size of cell nuclei extracted from biopsy microscope images
For these exercises, load the data by setting your options and loading the libraries and data as shown in the code here:

```{r}
options(digits = 3)
library(matrixStats)
library(ggplot2)
library(tidyverse)
library(caret)
library(dslabs)
library(dplyr)
library(tidybayes)
library(gam)
data(brca)
```

#### **Question 1: Dimensions and properties**  

How many samples are in the dataset?
```{r}
dim(brca$x)
```

How many predictors are in the matrix?
```{r}
dim(brca$x)
```

What proportion of the samples are malignant?
```{r}
mean(brca$y == "M")
```

Which column number has the highest mean?
```{r}
which.max(colMeans(brca$x))
```

Which column number has the lowest standard deviation?
```{r}
which.min(colSds(brca$x))
```

#### **Question 2: Scaling the matrix**  
Use sweep() two times to scale each column: subtract the column mean, then divide by the column standard deviation.

After scaling, what is the standard deviation of the first column?

```{r}
sweep_1 <- sweep(brca$x,2,colMeans(brca$x),'-')
x_scaled <- sweep(sweep_1, 2, colSds(brca$x), FUN = '/')
sd(x_scaled[,1])
```

After scaling, what is the median value of the first column?
```{r}
median(x_scaled[,1]) 
```

#### **Question 3: Distance**  
Calculate the distance between all samples using the scaled matrix.

What is the average distance between the first sample, which is benign, and other benign samples?
```{r}
d<- as.matrix(dist(x_scaled))
b_ind <- which(brca$y == 'B')
mean((d)[1:357])
#4.42. so d[1] is 0 and total mean distance of [1:357] is 4.4.
# distance b/w first B and all other Bs' is 4.4 - 0 = 4.4
mean((d)[1,b_ind])
```

What is the average distance between the first sample and malignant samples?
```{r}
m_ind <- which(brca$y == 'M')
mean((d)[358:569]) #=  7.12

# d[1] = 0, mean od Ms' = 7.12. so distance is 7.12 - 0 = 7.12
mean((d)[1,m_ind])
```

#### **Question 4: Heatmap of features**  
Make a heatmap of the relationship between features using the scaled matrix.

Which of these heatmaps is correct?
```{r}

d_features <- as.matrix (dist(t(x_scaled)))
heatmap((d_features),labRow = NA, labCol = NA)
```

#### **Question 5: Hierarchical clustering**  
Perform hierarchical clustering on the 30 features. Cut the tree into 5 groups.

All but one of the answer options are in the same group.

Which is in a different group?

```
clust <- hclust(d_features)
h_groups <- cutree(clust, k=5)
split(names(h_groups),h_groups)
```

#### **Question 6: PCA: proportion of variance**  
Perform a principal component analysis of the scaled matrix.

What proportion of variance is explained by the first principal component?

```{r}
pc <- prcomp(x_scaled)
# summary(pc)
# plot(summary(pc)$importance[2,]) from the plot you can see that the
#1st component is plotted somewhere above 0.4 so you can estimate

# What proportion of variance is explained by the first principal component?
variance <- cumsum(pc$sdev^2/sum(pc$sdev^2))
variance[1]
```
How many principal components are required to explain at least 90% of the variance?
```{r}
variance[1:7]
# variance[1:7]
# [1] 0.443 0.632 0.726 0.792 0.847 0.888 0.910 goes above 0.9 at 7

```

#### **Question 7: PCA: plotting PCs**  
Plot the first two principal components with color representing tumor type (benign/malignant).

Which of the following is true?
```{r}
df_1<- data.frame(pc$x[,1:2], type = brca$y)
#method 1
PC1 <- pc$x[,1]
PC2 <- pc$x[,2]
tumor <- brca$y

qplot(PC1, PC2, col=tumor)
```

```{r}
#method 2
data.frame(pc$x[,1:2], type = brca$y) %>%
  ggplot(aes(PC1, PC2, color = type)) +
  geom_point()
```

#### **Question 8: PCA: PC boxplot**  
Make a boxplot of the first 10 PCs grouped by tumor type.

Which PCs are significantly different enough by tumor type that there is no overlap in the interquartile ranges (IQRs) for benign and malignant samples?

```{r}
data <- data.frame(pc$x[,1:10], type = brca$y)

data %>% group_by(type) %>% boxplot(data)
  
data %>%
  gather(key = "PC", value = "value", -type) %>%
  ggplot(aes(PC, value, fill = type)) +
  geom_boxplot()
```

```{r}
set.seed(1, sample.kind = "Rounding")    # if using R 3.6 or later
test_index <- createDataPartition(brca$y, times = 1, p = 0.2, list = FALSE)
test_x <- x_scaled[test_index,]
test_y <- brca$y[test_index]
train_x <- x_scaled[-test_index,]
train_y <- brca$y[-test_index]
```

#### **Question 9: Training and test sets**  
Check that the training and test sets have similar proportions of benign and malignant tumors.

What proportion of the training set is benign?
```{r}
mean(train_y == "B")
```

What proportion of the test set is benign?
```{r}
mean(test_y == "B")
```

#### **Question 10a: K-means Clustering**  
The predict_kmeans() function defined here takes two arguments - a matrix of observations x and a k-means object k - and assigns each row of x to a cluster from k.

```{r}
set.seed(3, sample.kind = "Rounding")
k<- kmeans(train_x,centers = 2)
predict_kmeans <- function(x, k) {
    centers <- k$centers    # extract cluster centers
    # calculate distance to cluster centers
    distances <- sapply(1:nrow(x), function(i){
                        apply(centers, 1, function(y) dist(rbind(x[i,], y)))
                 })
  max.col(-t(distances))  # select cluster with min distance to center
}
```

Set the seed to 3. Perform k-means clustering on the training set with 2 centers and assign the output to k. Then use the predict_kmeans() function to make predictions on the test set.

What is the overall accuracy?
```{r}
predict_test <- predict_kmeans(test_x,k)
kmeans_preds <- as.factor(ifelse(predict_test == 1, "B","M"))
mean(kmeans_preds == test_y)
#confusionMatrix(test_y,predict_test)
```

#### **Question 10b: K-means Clustering**  
What proportion of benign tumors are correctly identified?
```{r}
library(caret)
sensitivity(factor(kmeans_preds), test_y, positive = "B")
```

What proportion of malignant tumors are correctly identified?
```{r}
sensitivity(factor(kmeans_preds), test_y, positive = "M")
```

#### **Question 11: Logistic regression model**  
```{r,warning=FALSE}
df <- data.frame(x = train_x, y= train_y)
df_test <- data.frame(x=test_x, y=test_y)

fit_glm <- train(y~., method = "glm",data = df)
y_hat <- predict(fit_glm,df_test)
mean(y_hat == df_test$y)
```

```{r,warning=FALSE}
#actual method
train_glm <- train(train_x, train_y,
                   method = "glm")
glm_preds <- predict(train_glm, test_x)
mean(glm_preds == test_y)
```

#### **Question 12: LDA and QDA models**  
Train an LDA model and a QDA model on the training set. Make predictions on the test set using each model.

What is the accuracy of the LDA model on the test set?

```{r,warning=FALSE}
train_lda <- train(train_x, train_y,
                  method = "lda")
lda_preds <- predict(train_lda, test_x)
mean(lda_preds == test_y)

```

What is the accuracy of the QDA model on the test set?

```{r,warning=FALSE}
train_qda <- train(train_x, train_y,
                   method = "qda")
qda_preds <- predict(train_qda, test_x)
mean(qda_preds == test_y)

```

#### **Question 13: Loess model**  
Set the seed to 5, then fit a loess model on the training set with the caret package. You will need to install the gam package if you have not yet done so. Use the default tuning grid. This may take several minutes; ignore warnings. Generate predictions on the test set.

What is the accuracy of the loess model on the test set?
```{r, warning=FALSE}
set.seed(5, sample.kind = "Rounding")
train_loess <- train(train_x, train_y,
                   method = "gamLoess")


loess_preds <- predict(train_loess, test_x)
mean(loess_preds == test_y)
```

#### **Question 14: K-nearest neighbors model**  
Set the seed to 7, then train a k-nearest neighbors model on the training set using the caret package. Try odd values of  k  from 3 to 21 (use tuneGrid with seq(3, 21, 2)). Use the final model to generate predictions on the test set.

What is the final value of  k  used in the model?
What is the accuracy of the kNN model on the test set?

```{r, warning = FALSE}
set.seed(7,sample.kind = "Rounding")
train_knn <- train(train_x, train_y,
                   method = "knn",
                   tuneGrid = data.frame(k = seq(3,21,2)))
ggplot(train_knn)
knn_preds <- predict(train_knn, test_x)
mean(knn_preds == test_y)
```

#### **Question 15a: Random forest model**  
Set the seed to 9, then train a random forest model on the training set using the caret package. Test mtry values of 3, 5, 7 and 9. Use the argument importance = TRUE so that feature importance can be extracted. Generate predictions on the test set.

What value of mtry gives the highest accuracy?
What is the accuracy of the random forest model on the test set?
What is the most important variable in the random forest model?

```{r, warning = FALSE}
set.seed(9, sample.kind = "Rounding")
train_rf <- train(train_x, train_y,
                   method = "rf",
                   tuneGrid = data.frame(mtry = seq(3,9,2)),
                   importance = TRUE)
ggplot(train_rf)
rf_preds <- predict(train_rf, test_x)
mean(rf_preds == test_y)
varImp(train_rf)
```

#### **Question 15b: Random forest model**  
Consider the top 10 most important variables in the random forest model.

Which set of features is most important for determining tumor type?

```{r}
varImp(train_rf)
```


#### **Question 16a: Creating an ensemble**  
Create an ensemble using the predictions from the 7 models created in the previous exercises: k-means, logistic regression, LDA, QDA, loess, k-nearest neighbors, and random forest. Use the ensemble to generate a majority prediction of the tumor type (if most models suggest the tumor is malignant, predict malignant).

What is the accuracy of the ensemble prediction?

```{r, warning= FALSE}
#METHOD1 (disclaimer - acc not right but accepted by edx. don't know how to include kmeans)

models <- c("glm", "lda", "qda", "gamLoess", "knn","rf")
set.seed(1, sample.kind = "Rounding")

fits_1 <- lapply(models, function(model){ 
  print(model)
  train(train_x, train_y, method = model)
}) 

preds_1 <- sapply(fits_1, function(object) 
  predict(object, newdata = test_x))
class(preds_1)

# you can find answer for 16B from here. shows LDA has 0.991 accuracy - the highest
acc <- colMeans(preds_1 == test_y)
mean(acc)
acc

votes <- rowMeans(preds_1 == "M")
y_hat <- ifelse(votes > 0.5 , "M", "B")
mean(y_hat == test_y)

```

```
ensemble <- cbind(glm = glm_preds == "M", lda = lda_preds == "M", qda = qda_preds == "M", loess = loess_preds == "M", rf = rf_preds == "M", knn = knn_preds == "M", kmeans = kmeans_preds == "M")

ensemble_preds <- ifelse(rowMeans(ensemble) > 0.5, "M", "B")
mean(ensemble_preds == test_y)
```

```
#method 3 16.1
ensemble <- cbind(glm = glm_preds, lda = lda_preds, qda = qda_preds, loess = loess_preds, rf = rf_preds, knn = knn_preds, kmeans = kmeans_preds)

ensemble_preds <- ifelse(rowMeans(ensemble==1)>0.5, "K", "N")


y_hat <- ifelse(ensemble_preds == "K" , "M", "B")

mean(y_hat == test_y)
```


#### **Question 16b: Creating an ensemble**  
Make a table of the accuracies of the 7 models and the accuracy of the ensemble model.

Which of these models has the highest accuracy?

```
models <- c("K means", "Logistic regression", "LDA", "QDA", "Loess", "K nearest neighbors", "Random forest", "Ensemble")
accuracy <- c(mean(kmeans_preds == test_y),
              mean(glm_preds == test_y),
              mean(lda_preds == test_y),
              mean(qda_preds == test_y),
              mean(loess_preds == test_y),
              mean(knn_preds == test_y),
              mean(rf_preds == test_y),
              mean(ensemble_preds == test_y))
data.frame(Model = models, Accuracy = accuracy)
```

