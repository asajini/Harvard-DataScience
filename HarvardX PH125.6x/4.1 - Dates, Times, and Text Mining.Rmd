---
title: "4 - Dates, Times, and Text Mining"
author: "Sajini Arumugam"
output: html_document
weight: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Assessment Part 1: Dates, Times, and Text Mining

This assessment reviews several concepts about dates, times, and text mining. In part 1 on this page, you will practice extracting and manipulating dates in real datasets. In part 2 on the next page, you will walk through a sentiment analysis of a novel using steps covered in the previous section.

Use the following libraries and options for coding questions:

library(dslabs)
library(lubridate)
options(digits = 3) 

#### **Question 1**  
Which of the following is the standard ISO 8601 format for dates?

- MM-DD-YY
- YYYY-MM-DD    [*]
- YYYYMMDD
- YY-MM-DD

#### **Question 2**  

Which of the following commands could convert this string into the correct date format?

dates <- c("09-01-02", "01-12-07", "02-03-04")

- ymd(dates)
- mdy(dates)
- dmy(dates)
- It is impossible to know which format is correct without additional information.   [*]

#### **Question 3**  

Load the brexit_polls data frame from dslabs:
```
data(brexit_polls)
```

How many polls had a start date (startdate) in April (month number 4)?

```
sum(month(brexit_polls$startdate) == 4)
```

Use the round_date function on the enddate column with the argument unit="week". How many polls ended the week of 2016-06-12?

```
sum(round_date(brexit_polls$enddate, unit = "week") == "2016-06-12")
```

#### **Question 4**  

Use the weekdays function from lubridate to determine the weekday on which each poll ended (enddate).

On which weekday did the greatest number of polls end?
- Monday
- Tuesday
- Wednesday
- Thursday
- Friday
- Saturday
- Sunday    [*]

#### **Question 5**  
Load the movielens data frame from dslabs.

data(movielens)
This data frame contains a set of about 100,000 movie reviews. The timestamp column contains the review date as the number of seconds since 1970-01-01 (epoch time).

Convert the timestamp column to dates using the lubridate as_datetime function.

Which year had the most movie reviews?

```
dates <- as_datetime(movielens$timestamp)
reviews_by_year <- table(year(dates))    # count reviews by year
names(which.max(reviews_by_year))    # name of year with most reviews
```

Which hour of the day had the most movie reviews?

```
reviews_by_hour <- table(hour(dates))    # count reviews by hour
names(which.max(reviews_by_hour))    # name of hour with most reviews
```
***

In this part of the assessment, you will walk through a basic text mining and sentiment analysis task.

Project Gutenberg is a digital archive of public domain books. The R package gutenbergr facilitates the importation of these texts into R. We will combine this with the tidyverse and tidytext libraries to practice text mining.

Use these libraries and options:

library(tidyverse)
library(gutenbergr)
library(tidytext)
options(digits = 3)
You can see the books and documents available in gutenbergr like this:

gutenberg_metadata

#### **Question 6**  

Use str_detect to find the ID of the novel Pride and Prejudice.

How many different ID numbers are returned?

```
gutenberg_metadata %>%
    filter(str_detect(title, "Pride and Prejudice"))
```

#### **Question 7**  

Notice that there are several versions of the book. The gutenberg_works function filters this table to remove replicates and include only English language works. Use this function to find the ID for Pride and Prejudice.

What is the correct ID number?
```
gutenberg_works(title == "Pride and Prejudice")$gutenberg_id
```

#### **Question 8**  

Use the gutenberg_download function to download the text for Pride and Prejudice. Use the tidytext package to create a tidy table with all the words in the text. Save this object as words.

How many words are present in the book?

```
book <- gutenberg_download(1342)
words <- book %>%
  unnest_tokens(word, text)
nrow(words)
```

#### **Question 9**  

Remove stop words from the words object. Recall that stop words are defined in the stop_words data frame from the tidytext package.

How many words remain?

```
words <- words %>% anti_join(stop_words)
nrow(words)
```

#### **Question 10**  

After removing stop words, detect and then filter out any token that contains a digit from words.

How many words remain?

```
words <- words %>%
  filter(!str_detect(word, "\\d"))
nrow(words)
```

#### **Question 11**  

Analyze the most frequent words in the novel after removing stop words and tokens with digits.

How many words appear more than 100 times in the book?

```
words %>%
    count(word) %>%
    filter(n > 100) %>%
    nrow()
```

What is the most common word in the book?

```
words %>%
    count(word) %>%
    top_n(1, n) %>%
    pull(word)
```

How many times does that most common word appear?

```
words %>%
    count(word) %>%
    top_n(1, n) %>%
    pull(n)
```

#### **Question 12**  

Define the afinn lexicon:
```
afinn <- get_sentiments("afinn")
```

Note that this command will trigger a question in the R Console asking if you want to download the AFINN lexicon. Press 1 to select "Yes" (if using RStudio, enter this in the Console tab).

Use this afinn lexicon to assign sentiment values to words. Keep only words that are present in both words and the afinn lexicon. Save this data frame as afinn_sentiments.

How many elements of words have sentiments in the afinn lexicon?

```
afinn_sentiments <- inner_join(afinn, words)
nrow(afinn_sentiments)
```

What proportion of words in afinn_sentiments have a positive value?

```
mean(afinn_sentiments$value > 0)
```

How many elements of afinn_sentiments have a value of 4?
```
sum(afinn_sentiments$value == 4)
```
***

### Puerto Rico Hurricane Mortality: Part 1

#### **Question 1**  
In the extdata directory of the dslabs package, you will find a PDF file containing daily mortality data for Puerto Rico from Jan 1, 2015 to May 31, 2018. You can find the file like this:

```
fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf", package="dslabs")
```

Find and open the file or open it directly from RStudio. On a Mac, you can type:
```
system2("open", args = fn)
```

and on Windows, you can type:

```
system("cmd.exe", input = paste("start", fn))
```

Which of the following best describes this file?

- It is a table. Extracting the data will be easy.
- It is a report written in prose. Extracting the data will be impossible.
- It is a report combining graphs and tables. Extracting the data seems possible.   [*]
- It shows graphs of the data. Extracting the data will be difficult.

#### **Question 2**  

We are going to create a tidy dataset with each row representing one observation. The variables in this dataset will be year, month, day and deaths.

Use the pdftools package to read in fn using the pdf_text function. Store the results in an object called txt.

Describe what you see in txt.

- A table with the mortality data.
- A character string of length 12. Each entry represents the text in each page. The mortality data is in there somewhere.    [*]
- A character string with one entry containing all the information in the PDF file.
- An html document.

#### **Question 3**  

Extract the ninth page of the PDF file from the object txt, then use the str_split function from the stringr package so that you have each line in a different entry. The new line character is \n. Call this string vector x.

Look at x. What best describes what you see?

- It is an empty string.
- I can see the figure shown in page 1.
- It is a tidy table.
- I can see the table! But there is a bunch of other stuff we need to get rid of.    [*]

What kind of object is x?  

How many entries does x have?

#### **Question 4**  
Define s to be the first entry of the x object.

What kind of object is s?  
character vector  
```
s <- x[[1]]
class(s)
```

How many entries does s have? 
```
length(s)
```

#### **Question 5**  

When inspecting the string we obtained above, we see a common problem: white space before and after the other characters. Trimming is a common first step in string processing. These extra spaces will eventually make splitting the strings hard so we start by removing them.

We learned about the command str_trim that removes spaces at the start or end of the strings. Use this function to trim s and assign the result to s again.

After trimming, what single character is the last character of element 1 of s?
```
s <- str_trim(s)
s[1] 
```

#### **Question 6**  

We want to extract the numbers from the strings stored in s. However, there are a lot of non-numeric characters that will get in the way. We can remove these, but before doing this we want to preserve the string with the column header, which includes the month abbreviation.

Use the str_which function to find the row with the header. Save this result to header_index. Hint: find the first string that matches the pattern "2015" using the str_which function.

What is the value of header_index?

```
header_index <- str_which(s, "2015")[1]
header_index
```

#### **Question 7**  

We want to extract two objects from the header row: month will store the month and header will store the column names.

Save the content of the header row into an object called header, then use str_split to help define the two objects we need.

What is the value of month?
Use header_index to extract the row. The separator here is one or more spaces. Also, consider using the simplify argument.

```
tmp <- str_split(s[header_index], "\\s+", simplify = TRUE)
month <- tmp[1]
header <- tmp[-1]
month
```

What is the third value in header?

```
header[3]
```

Puerto Rico Hurricane Mortality: Part 2

#### **Question 8**  

Notice that towards the end of the page defined by s you see a "Total" row followed by rows with other summary statistics. Create an object called tail_index with the index of the "Total" entry.

What is the value of tail_index?

```
tail_index  <- str_which(s, "Total")
tail_index
```

#### **Question 9**  

Because our PDF page includes graphs with numbers, some of our rows have just one number (from the y-axis of the plot). Use the str_count function to create an object n with the count of numbers in each row.

How many rows have a single number in them?
You can write a regex for a number like this \\d+.

```
n <- str_count(s, "\\d+")
sum(n == 1)
```

#### **Question 10**  

We are now ready to remove entries from rows that we know we don't need. The entry header_index and everything before it should be removed. Entries for which n is 1 should also be removed, and the entry tail_index and everything that comes after it should be removed as well.

How many entries remain in s?

```
out <- c(1:header_index, which(n==1), tail_index:length(s))
s <- s[-out]
length(s)
```

#### **Question 11**  

Now we are ready to remove all text that is not a digit or space. Do this using regular expressions (regex) and the str_remove_all function.

In regex, using the ^ inside the square brackets [] means not, like the ! means not in !=. To define the regex pattern to catch all non-numbers, you can type [^\\d]. But remember you also want to keep spaces.

Which of these commands produces the correct output?

- s <- str_remove_all(s, "[^\\d]")
- s <- str_remove_all(s, "[\\d\\s]")
- s <- str_remove_all(s, "[^\\d\\s]")   [*]
- s <- str_remove_all(s, "[\\d]")

#### **Question 12**  

Use the str_split_fixed function to convert s into a data matrix with just the day and death count data:

s <- str_split_fixed(s, "\\s+", n = 6)[,1:5]
Now you are almost ready to finish. Add column names to the matrix: the first column should be day and the next columns should be the header. Convert all values to numeric. Also, add a column with the month. Call the resulting object tab.

What was the mean number of deaths per day in September 2015?

```
tab <- s %>% 
    as_data_frame() %>% 
    setNames(c("day", header)) %>%
    mutate_all(as.numeric)
mean(tab$"2015")
```

What is the mean number of deaths per day in September 2016?
```
mean(tab$"2016")
```

Hurricane María hit Puerto Rico on September 20, 2017. What was the mean number of deaths per day from September 1-19, 2017, before the hurricane hit?

```
mean(tab$"2017"[1:19])
```
What was the mean number of deaths per day from September 20-30, 2017, after the hurricane hit?

```
mean(tab$"2017"[20:30])
```

#### **Question 13**  

Finish it up by changing tab to a tidy format, starting from this code outline:
```
tab <- tab %>% _____(year, deaths, -day) %>%
    mutate(deaths = as.numeric(deaths))
tab
```

What code fills the blank to generate a data frame with columns named "day", "year" and "deaths"?
- separate
- unite
- gather    [*]
- spread

#### **Question 14**  

Make a plot of deaths versus day with color to denote year. Exclude 2018 since we have no data. Add a vertical line at day 20, the day that Hurricane María hit in 2017.

Which of the following are TRUE?
- September 2015 and 2016 deaths by day are roughly equal to each other.    [*]
- The day with the most deaths was the day of the hurricane: September 20, 2017.
- After the hurricane in September 2017, there were over 100 deaths per day every day for the rest of the month.    [*]
- No days before September 20, 2017 have over 100 deaths per day.   [*]

