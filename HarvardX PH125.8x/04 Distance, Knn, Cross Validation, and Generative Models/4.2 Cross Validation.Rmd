---
title: "4.2 - Cross Validation"
author: "Sajini Arumugam"
output: html_document
weight: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### **Comprehension Check: Cross-validation**

#### **Question 1**
Generate a set of random predictors and outcomes using the following code:
```{r}
library(dplyr)
set.seed(1996, sample.kind="Rounding")
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
```
Because x and y are completely independent, you should not be able to predict y using x with accuracy greater than 0.5. Confirm this by running cross-validation using logistic regression to fit the model. Because we have so many predictors, we selected a random sample x_subset. Use the subset when training the model.

Which code correctly performs this cross-validation?

```{r}
library(caret)
fit <- train(x_subset, y, method = "glm")
fit$results
```
 
#### **Question 2**  
Now, instead of using a random selection of predictors, we are going to search for those that are most predictive of the outcome. We can do this by comparing the values for the  y=1  group to those in the  y=0  group, for each predictor, using a t-test. You can do perform this step like this:


```{r}

library(genefilter)
tt <- colttests(x, y)

```

Which of the following lines of code correctly creates a vector of the p-values called pvals?

```{r}
pvals <- tt$p.value
```

#### **Question 3**  
Create an index ind with the column numbers of the predictors that were "statistically significantly" associated with y. Use a p-value cutoff of 0.01 to define "statistically significantly."

How many predictors survive this cutoff?

```{r}
ind <- which(pvals <= 0.01)
length(ind)
```

#### **Question 4**  
Now re-run the cross-validation after redefinining x_subset to be the subset of x defined by the columns showing "statistically significant" association with y.

What is the accuracy now?

```{r}
x_subset <- x[,ind]
fit <- train(x_subset, y, method = "glm")
fit$results
```

##### **Question 5**
Re-run the cross-validation again, but this time using kNN. Try out the following grid k = seq(101, 301, 25) of tuning parameters. Make a plot of the resulting accuracies.

Which code is correct?

```{r}
fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(101, 301, 25)))
ggplot(fit)
```

#### **Question 6**
In the previous exercises, we see that despite the fact that x and y are completely independent, we were able to predict y with accuracy higher than 70%. We must be doing something wrong then.

What is it?

- The function train() estimates accuracy on the same data it uses to train the algorithm.
- We are overfitting the model by including 100 predictors.
- We used the entire dataset to select the columns used in the model.       [*]
- The high accuracy is just due to random variability.


#### **Question 7**  
Use the train() function with kNN to select the best k for predicting tissue from gene expression on the tissue_gene_expression dataset from dslabs. Try k = seq(1,7,2) for tuning parameters. For this question, do not split the data into test and train sets (understand this can lead to overfitting, but ignore this for now).

What value of k results in the highest accuracy?

```{r}
library(dslabs)
data("tissue_gene_expression")
fit <- with(tissue_gene_expression, train(x, y, method = "knn", tuneGrid = data.frame( k = seq(1, 7, 2))))
ggplot(fit)
fit$results
```

***

### **Comprehension Check: Bootstrap**  

#### **Question 1**  
The createResample() function can be used to create bootstrap samples. For example, we can create the indexes for 10 bootstrap samples for the mnist_27 dataset like this:

```{r}
library(dslabs)
library(caret)
data(mnist_27)
set.seed(1995) # if R 3.6 or later, set.seed(1995, sample.kind="Rounding")
indexes <- createResample(mnist_27$train$y, 10)
```

How many times do 3, 4, and 7 appear in the first resampled index?

```{r}
sum(indexes[[1]] == 3)
sum(indexes[[1]] == 4)
sum(indexes[[1]] == 7)
```

#### **Question 2**
We see that some numbers appear more than once and others appear no times. This has to be this way for each dataset to be independent. Repeat the exercise for all the resampled indexes.

What is the total number of times that 3 appears in all of the resampled indexes?

```{r}
x = sapply(indexes, function(ind)
  {sum(ind== 3)
  })
sum(x)
```

#### **Question 3**  
A random dataset can be generated using the following code:

```{r}
y <- rnorm(100, 0, 1)
```

Estimate the 75th quantile, which we know is qnorm(0.75), with the sample quantile: quantile(y, 0.75).

Set the seed to 1 and perform a Monte Carlo simulation with 10,000 repetitions, generating the random dataset and estimating the 75th quantile each time. What is the expected value and standard error of the 75th quantile?

Expected value & Standard Error:

```{r}
set.seed(1, sample.kind="Rounding")
indexes <- createResample(y, 10)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```

#### **Question 4**  
In practice, we can't run a Monte Carlo simulation. Use the sample:
```{r, include=FALSE}
set.seed(1)
y <- rnorm(100, 0, 1)
```
```{r, include = TRUE}
set.seed(1, sample.kind="Rounding") 
indexes <- createResample(y, 10)
q_75_star <- sapply(indexes, function(ind){
	y_star <- y[ind]
	quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```


#### **Question 5**  
Repeat the exercise from Q4 but with 10,000 bootstrap samples instead of 10. Set the seed to 1.

Expected value & Standard Error:

```{r, include = TRUE}
library(dslabs)
library(caret)
set.seed(1, sample.kind = "Rounding")
y <- rnorm(100, 0, 1)
set.seed(1, sample.kind = "Rounding")
indexes <- createResample(y, 10000)
q_75_star <- sapply(indexes, function(ind){
  y_star <- y[ind]
  quantile(y_star, 0.75)
})
mean(q_75_star)
sd(q_75_star)
```

#### **Question 6**  
When doing bootstrap sampling, the simulated samples are drawn from the empirical distribution of the original data.

True or False: The bootstrap is particularly useful in situations in which a tractable variance formula exists.

- True
- False     [*]