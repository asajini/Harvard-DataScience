---
title: "2 - Linear Models"
author: "Sajini Arumugam"
output: html_document
weight : 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Question 1

Why is the number of home runs considered a confounder of the relationship between bases on balls and runs per game?
- Home runs is not a confounder of this relationship.
- Home runs are the primary cause of runs per game.
- The correlation between home runs and runs per game is stronger than the correlation between bases on balls and runs per game.
- Players who get more bases on balls also tend to have more home runs; in addition, home runs increase the points per game.    [*]


Question 2

As described in the videos, when we stratified our regression lines for runs per game vs. bases on balls by the number of home runs, what happened?
- The slope of runs per game vs. bases on balls within each stratum was reduced because we removed confounding by home runs.   [*]
- The slope of runs per game vs. bases on balls within each stratum was reduced because there were fewer data points.
- The slope of runs per game vs. bases on balls within each stratum increased after we removed confounding by home runs.
- The slope of runs per game vs. bases on balls within each stratum stayed about the same as the original slope.


Question 3

We run a linear model for sons’ heights vs. fathers’ heights using the Galton height data, and get the following results:

```
 > lm(son ~ father, data = galton_heights)

Call:
lm(formula = son ~ father, data = galton_heights)

Coefficients:
(Intercept)    father  
    35.71       0.50  
```

Interpret the numeric coefficient for "father."
- For every inch we increase the son’s height, the predicted father’s height increases by 0.5 inches.
- For every inch we increase the father’s height, the predicted son’s height grows by 0.5 inches.   [*]
- For every inch we increase the father’s height, the predicted son’s height is 0.5 times greater.

Question 4

We want the intercept term for our model to be more interpretable, so we run the same model as before but now we subtract the mean of fathers’ heights from each individual father’s height to create a new variable centered at zero.

```
galton_heights <- galton_heights %>%
    mutate(father_centered=father - mean(father))
```

We run a linear model using this centered fathers’ height variable.


> lm(son ~ father_centered, data = galton_heights)

Call:
lm(formula = son ~ father_centered, data = galton_heights)

Coefficients:
(Intercept)    father_centered  
    70.45          0.50  
    
Interpret the numeric coefficient for the intercept.
- The height of a son of a father of average height is 70.45 inches.   [*]
- The height of a son when a father’s height is zero is 70.45 inches.
- The height of an average father is 70.45 inches.


Question 5

Suppose we fit a multivariate regression model for expected runs based on BB and HR:

E[R|BB=x1,HR=x2]=β0+β1x1+β2x2 
Suppose we fix  BB=x1 . Then we observe a linear relationship between runs and HR with intercept of:

- β0 
- β0+β2x2 
- β0+β1x1 [*]
- β0+β2x1Suppose we fix  BB=x1 

Question 6

Which of the following are assumptions for the errors ϵi in a linear regression model?

- The  ϵi  are independent of each other[*]
- The  ϵi  have expected value 0[*]
- The variance of  ϵi  is a constant[*]


***

Assessment: Least Squares Estimates, part 1


Question 1

The following code was used in the video to plot RSS with  β0=25 .

```
beta1 = seq(0, 1, len=nrow(galton_heights))
results <- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %>% ggplot(aes(beta1, rss)) + geom_line() + 
  geom_line(aes(beta1, rss), col=2)
```

In a model for sons’ heights vs fathers’ heights, what is the least squares estimate (LSE) for  β1  if we assume  β^0  is 36?

- 0.65
- 0.5   [*]
- 0.2
- 12

Question 2

The least squares estimates for the parameters β0,β1,…,βn minimize the residual sum of squares.


Question 3

Load the Lahman library and filter the Teams data frame to the years 1961-2001. Run a linear model in R predicting the number of runs per game based on both the number of bases on balls per game and the number of home runs per game.

What is the coefficient for bases on balls?

- 0.39    [*]
- 1.56
- 1.74
- 0.027


Question 4

We run a Monte Carlo simulation where we repeatedly take samples of N = 100 from the Galton heights data and compute the regression slope coefficients for each sample:

```
B <- 1000
N <- 100
lse <- replicate(B, {
    sample_n(galton_heights, N, replace = TRUE) %>% 
    lm(son ~ father, data = .) %>% .$coef 
})

lse <- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) 
```

What does the central limit theorem tell us about the variables beta_0 and beta_1?

- They are approximately normally distributed.[*]
- The expected value of each is the true value of  β0  and  β1 (assuming the Galton heights data is a complete population).    [*]
- The central limit theorem does not apply in this situation.
- It allows us to test the hypothesis that  β0=0  and  β1=0 .


Question 5

In an earlier video, we ran the following linear model and looked at a summary of the results.

```
> mod <- lm(son ~ father, data = galton_heights)
> summary(mod)

Call:
lm(formula = son ~ father, data = galton_heights)

Residuals:
   Min     1Q  Median     3Q    Max 
-5.902  -1.405  0.092    1.342  8.092 

Coefficients:
                 Estimate  Std. Error  t value     Pr(>|t|)  
(Intercept)     35.7125     4.5174       7.91    2.8e-13 ***
father           0.5028     0.0653       7.70    9.5e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```

What null hypothesis is the second p-value (the one in the father row) testing?

-  β1=1 , where  β1  is the coefficient for the variable "father."
-  β1=0.503 , where  β1  is the coefficient for the variable "father."
-  β1=0 , where  β1  is the coefficient for the variable "father."    [*]

Question 6

Which R code(s) below would properly plot the predictions and confidence intervals for our linear model of sons’ heights?

```
galton_heights %>% ggplot(aes(father, son)) +
    geom_point() +
    geom_smooth()
```

```
[*]
galton_heights %>% ggplot(aes(father, son)) +
    geom_point() +
    geom_smooth(method = "lm")
```

```
[*]
model <- lm(son ~ father, data = galton_heights)
predictions <- predict(model, interval = c("confidence"), level = 0.95)
data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
    geom_line(color = "blue", size = 1) + 
    geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=0.2) + 
    geom_point(data = galton_heights, aes(x = father, y = son))
```

```
model <- lm(son ~ father, data = galton_heights)
predictions <- predict(model)
data <- as.tibble(predictions) %>% bind_cols(father = galton_heights$father)

ggplot(data, aes(x = father, y = fit)) +
    geom_line(color = "blue", size = 1) + 
    geom_point(data = galton_heights, aes(x = father, y = son))
```

In Questions 7 and 8, you'll look again at female heights from GaltonFamilies.

Define female_heights, a set of mother and daughter heights sampled from GaltonFamilies, as follows:

```
set.seed(1989, sample.kind="Rounding") 
library(HistData)
data("GaltonFamilies")
options(digits = 3)    # report 3 significant digits
female_heights <- GaltonFamilies %>%     
    filter(gender == "female") %>%     
    group_by(family) %>%     
    sample_n(1) %>%     
    ungroup() %>%     
    select(mother, childHeight) %>%     
    rename(daughter = childHeight) 
```

Question 7

Fit a linear regression model predicting the mothers' heights using daughters' heights.

What is the slope of the model?

```
fit <- lm(mother ~ daughter, data = female_heights)
fit$coef[2]
```

What the intercept of the model?

```
fit$coef[1]
```

Question 8

Predict mothers' heights using the model.

What is the predicted height of the first mother in the dataset?

```
predict(fit)[1]
```

What is the actual height of the first mother in the dataset?

```
female_heights$mother[1]
```

Question 9

Now compute a similar table but with rates computed over 1999-2001. Keep only rows from 1999-2001 where players have 100 or more plate appearances, then calculate the average single rate (mean_singles) and average BB rate (mean_bb) per player over those three seasons.

How many players had a single rate mean_singles of greater than 0.2 per plate appearance over 1999-2001?

```
library(Lahman)
bat_02 <- Batting %>% filter(yearID == 2002) %>%
    mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %>%
    filter(pa >= 100) %>%
    select(playerID, singles, bb)
```

```
bat_99_01 <- Batting %>% filter(yearID %in% 1999:2001) %>%
    mutate(pa = AB + BB, singles = (H - X2B - X3B - HR)/pa, bb = BB/pa) %>%
    filter(pa >= 100) %>%
    group_by(playerID) %>%
    summarize(mean_singles = mean(singles), mean_bb = mean(bb))
sum(bat_99_01$mean_singles > 0.2)
```

How many players had a BB rate mean_bb of greater than 0.2 per plate appearance over 1999-2001?

```
sum(bat_99_01$mean_bb > 0.2)
```

Question 10

Use inner_join to combine the bat_02 table with the table of 1999-2001 rate averages you created in the previous question.

What is the correlation between 2002 singles rates and 1999-2001 average singles rates?

```
dat <- inner_join(bat_02, bat_99_01)
cor(dat$singles, dat$mean_singles)
```

What is the correlation between 2002 BB rates and 1999-2001 average BB rates?

```
cor(dat$bb, dat$mean_bb)
```


Question 11

Make scatterplots of mean_singles versus singles and mean_bb versus bb.

Are either of these distributions bivariate normal?

- Neither distribution is bivariate normal.
- singles and mean_singles are bivariate normal, but bb and mean_bb are not.
- bb and mean_bb are bivariate normal, but singles and mean_singles are not.
- Both distributions are bivariate normal.    [*]

Question 12

Fit a linear model to predict 2002 singles given 1999-2001 mean_singles.

What is the coefficient of mean_singles, the slope of the fit?

```
fit_singles <- lm(singles ~ mean_singles, data = dat)
fit_singles$coef[2]
```

Fit a linear model to predict 2002 bb given 1999-2001 mean_bb.

What is the coefficient of mean_bb, the slope of the fit?

```
fit_bb <- lm(bb ~ mean_bb, data = dat)
fit_bb$coef[2]
```

***

Assessment: Tibbles, do, and broom, part 1

Question 1

As seen in the videos, what problem do we encounter when we try to run a linear model on our baseball data, grouping by home runs?
- There is not enough data in some levels to run the model.
- The lm function does not know how to handle grouped tibbles.    [*]
- The results of the lm function cannot be put into a tidy format.


Question 2

Tibbles are similar to what other class in R?
Vectors
Matrices
Data frames   [*]
Lists


Question 3

What are some advantages of tibbles compared to data frames?
Select ALL that apply.

Tibbles display better.
If you subset a tibble, you always get back a tibble.
Tibbles can have complex entries.
Tibbles can be grouped.
[all of the above]


Question 4

What are two advantages of the do command, when applied to the tidyverse?
Select TWO.

It is faster than normal functions.
It returns useful error messages.
It understands grouped tibbles.   [*]
It always returns a data.frame.   [*]


Question 5

You want to take the tibble dat, which we used in the video on the do function, and run the linear model R ~ BB for each strata of HR. Then you want to add three new columns to your grouped tibble: the coefficient, standard error, and p-value for the BB term in the model.

You’ve already written the function get_slope, shown below.

```
get_slope <- function(data) {
  fit <- lm(R ~ BB, data = data)
  sum.fit <- summary(fit)

  data.frame(slope = sum.fit$coefficients[2, "Estimate"], 
             se = sum.fit$coefficients[2, "Std. Error"],
             pvalue = sum.fit$coefficients[2, "Pr(>|t|)"])
}
```
What additional code could you write to accomplish your goal?

```
dat %>% 
  group_by(HR) %>% 
  do(get_slope(.))
```

Question 6

The output of a broom function is always what?

A data.frame    [*]
A list
A vector


Question 7

You want to know whether the relationship between home runs and runs per game varies by baseball league. You create the following dataset:

```
dat <- Teams %>% filter(yearID %in% 1961:2001) %>%
  mutate(HR = HR/G,
         R = R/G) %>%
  select(lgID, HR, BB, R) 
```
What code would help you quickly answer this question?

```
dat %>% 
  group_by(lgID) %>% 
  do(tidy(lm(R ~ HR, data = .), conf.int = T)) %>% 
  filter(term == "HR") 
```


We have investigated the relationship between fathers' heights and sons' heights. But what about other parent-child relationships? Does one parent's height have a stronger association with child height? How does the child's gender affect this relationship in heights? Are any differences that we observe statistically significant?

The galton dataset is a sample of one male and one female child from each family in the GaltonFamilies dataset. The pair column denotes whether the pair is father and daughter, father and son, mother and daughter, or mother and son.

Create the galton dataset using the code below:

```{r}
library(tidyverse)
library(HistData)
data("GaltonFamilies")
set.seed(1, sample.kind = "Rounding") 
galton <- GaltonFamilies %>%
    group_by(family, gender) %>%
    sample_n(1) %>%
    ungroup() %>% 
    gather(parent, parentHeight, father:mother) %>%
    mutate(child = ifelse(gender == "female", "daughter", "son")) %>%
    unite(pair, c("parent", "child"))

galton
```


