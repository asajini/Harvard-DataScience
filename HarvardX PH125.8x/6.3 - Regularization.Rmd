---
title: "6.3 - Regularization"
author: "Sajini Arumugam"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Comprehension Check: Regularization

The exercises in Q1-Q8 work with a simulated dataset for 1000 schools. This pre-exercise setup walks you through the code needed to simulate the dataset.

If you have not done so already since the Titanic Exercises, please restart R or reset the number of digits that are printed with options(digits=7).

An education expert is advocating for smaller schools. The expert bases this recommendation on the fact that among the best performing schools, many are small schools. Let's simulate a dataset for 1000 schools. First, let's simulate the number of students in each school, using the following code:

```{r}
set.seed(1986, sample.kind="Rounding")
n <- round(2^rnorm(1000, 8, 1))
```

Now let's assign a true quality for each school that is completely independent from size. This is the parameter we want to estimate in our analysis. The true quality can be assigned using the following code:

```{r}
set.seed(1, sample.kind="Rounding")
mu <- round(80 + 2*rt(1000, 5))
range(mu)
schools <- data.frame(id = paste("PS",1:1000),
                      size = n,
                      quality = mu,
                      rank = rank(-mu))
```

We can see the top 10 schools using this code: 

```{r}
library(dplyr)
schools %>% top_n(10, quality) %>% arrange(desc(quality))
```
Now let's have the students in the school take a test. There is random variability in test taking, so we will simulate the test scores as normally distributed with the average determined by the school quality with a standard deviation of 30 percentage points. This code will simulate the test scores:

```{r}
set.seed(1, sample.kind="Rounding")
mu <- round(80 + 2*rt(1000, 5))

scores <- sapply(1:nrow(schools), function(i){
       scores <- rnorm(schools$size[i], schools$quality[i], 30)
       scores
})
schools <- schools %>% mutate(score = sapply(scores, mean))
```

#### **Question 1**  
What are the top schools based on the average score? Show just the ID, size, and the average score.

Report the ID of the top school and average score of the 10th school.

What is the ID of the top school?

```{r}
score_based <- schools %>% select(id, size, score)%>%group_by(id) %>%
  summarize(score = score, size = size)%>% 
  arrange(desc(score))%>%
  top_n(10,score)
  score_based
top_10 <- score_based$score
```

#### **Question 2**  

Compare the median school size to the median school size of the top 10 schools based on the score.

What is the median school size overall?
What is the median school size of the of the top 10 schools based on the score?

```{r}
median(schools$size)
median(score_based$size)
```

#### **Question 3**  
According to this analysis, it appears that small schools produce better test scores than large schools. Four out of the top 10 schools have 100 or fewer students. But how can this be? We constructed the simulation so that quality and size were independent. Repeat the exercise for the worst 10 schools.

What is the median school size of the bottom 10 schools based on the score?

```{r}
last_10 <- schools %>% select(id, size, score)%>%group_by(id) %>%
  summarize(score = score, size = size)%>% 
  arrange((score))%>%
  top_n(-10,score)
last_10

median(last_10$size)
```

#### **Question 4**  
From this analysis, we see that the worst schools are also small. Plot the average score versus school size to see what's going on. Highlight the top 10 schools based on the true quality.

What do you observe?

```{r}
library(ggplot2)
highlight <- data.frame(top_10)
highlight
schools %>% ggplot(aes(size,score, color = score))+
  geom_point(alpha=0.3) +
  geom_point(data= filter(schools, rank<=10), col = 2)
```

#### **Question 5**  
Let's use regularization to pick the best schools. Remember regularization shrinks deviations from the average towards 0. To apply regularization here, we first need to define the overall average for all schools, using the following code:

```{r}
overall <- mean(sapply(scores, mean))
```

Then, we need to define, for each school, how it deviates from that average.

Write code that estimates the score above the average for each school but dividing by  n+α  instead of  n , with  n  the school size and  α  a regularization parameter. Try  α=25 .

What is the ID of the top school with regularization?

```{r}
alpha <- 25
reg_score <- sapply(scores, function(i) overall+sum(i-overall)/(length(i)+alpha))
schools %>% mutate(reg_score=reg_score)%>%
  top_n(10,reg_score)%>% arrange(desc(reg_score))
```

```{r}
j<-overall +(schools$score-overall)* schools$size/(schools$size+alpha)

```

#### **Question 6**  
Notice that this improves things a bit. The number of small schools that are not highly ranked is now lower. Is there a better  α ? Using values of  α  from 10 to 250, find the  α  that minimizes the RMSE.

RMSE=11000∑i=11000(quality−estimate)2−−−−−−−−−−−−−−−−−−−−−−−−⎷ 
What value of  α  gives the minimum RMSE?

```{r}
set.seed(1,sample.kind = "Rounding")
alphas<-seq(10,250)
rmse <- sapply(alphas, function(l){
  reg_score <- sapply(scores, function(i) overall+sum(i-overall)/(length(i)+l))
  sqrt(mean((reg_score-schools$quality)^2))
})
tg <- data.frame(rmse,alphas)
tg%>% arrange(desc(rmse))
  plot(alphas,rmse)
  alphas[which.min(rmse)] 
```

#### **Question 7**  
Rank the schools based on the average obtained with the best  α . Note that no small school is incorrectly included.

What is the ID of the top school now?
What is the regularized average score of the 10th school now?

```{r}
alpha <- 135
  reg_score <- sapply(scores, function(i) overall+sum(i-overall)/(length(i)+alpha))
  schools %>% mutate(reg_score=reg_score)%>%
    top_n(10,reg_score)%>% arrange(desc(reg_score))
```

#### **Question 8**  
A common mistake made when using regularization is shrinking values towards 0 that are not centered around 0. For example, if we don't subtract the overall average before shrinking, we actually obtain a very similar result. Confirm this by re-running the code from the exercise in Q6 but without removing the overall mean.

What value of  α  gives the minimum RMSE here?

```{r}
alphas<-seq(10,250)
  rmse <- sapply(alphas, function(l){
    reg_score <- sapply(scores, function(i) sum(i)/(length(i)+l))
    sqrt(mean((reg_score - schools$quality)^2))
  })

  alphas[which.min(rmse)]
```

### Comprehension Check: Matrix Factorization

In this exercise set, we will be covering a topic useful for understanding matrix factorization: the singular value decomposition (SVD). SVD is a mathematical result that is widely used in machine learning, both in practice and to understand the mathematical properties of some algorithms. This is a rather advanced topic and to complete this exercise set you will have to be familiar with linear algebra concepts such as matrix multiplication, orthogonal matrices, and diagonal matrices.

The SVD tells us that we can decompose an  N×p  matrix  Y  with  p<N  as 

Y=UDV⊤ 
with  U  and  V  orthogonal of dimensions  N×p  and  p×p  respectively and  D  a  p×p  diagonal matrix with the values of the diagonal decreasing: 

d1,1≥d2,2≥…dp,p 
In this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:

```{r}
set.seed(1987)
#if using R 3.6 or later, use `set.seed(1987, sample.kind="Rounding")` instead
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) 
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m <- m[order(rowMeans(m), decreasing = TRUE),]
y <- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3)
colnames(y) <- c(paste(rep("Math",k), 1:k, sep="_"),
                 paste(rep("Science",k), 1:k, sep="_"),
                 paste(rep("Arts",k), 1:k, sep="_"))
```

Our goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just a random independent numbers. Are all students just about as good? Does being good in one subject  imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this  100×24  dataset. 

#### **Question 1**  
You can visualize the 24 test scores for the 100 students by plotting an image:

```{r}
my_image <- function(x, zlim = range(x), ...){
	colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
	cols <- 1:ncol(x)
	rows <- 1:nrow(x)
	image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
			xlab="", ylab="",  col = colors, zlim = zlim, ...)
	abline(h=rows + 0.5, v = cols + 0.5)
	axis(side = 1, cols, colnames(x), las = 2)
}

my_image(y)
```

- The test scores are all independent of each other.
- The students that are good at math are not good at science.
- The students that are good at math are not good at arts.
- The students that test well are at the top of the image and there seem to be three groupings by subject.   [*]
- The students that test well are at the bottom of the image and there seem to be three groupings by subject.

#### **Question 2**  
You can examine the correlation between the test scores directly like this:

```{r}
my_image(cor(y), zlim = c(-1,1))
range(cor(y))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)
```
Which of the following best describes what you see?

- The test scores are independent.
- Test scores in math and science are highly correlated but scores in arts are not.
- There is high correlation between tests in the same subject but no correlation across subjects.
- There is correlation among all tests, but higher if the tests are in science and math and even higher within each subject. [*]

#### **Question 3**  
Remember that orthogonality means that  U⊤U  and  V⊤V  are equal to the identity matrix. This implies that we can also rewrite the decomposition as

YV=UD or U⊤Y=DV⊤ 
We can think of  YV  and  U⊤V  as two transformations of  Y  that preserve the total variability of  Y  since  U  and  V  are orthogonal.

Use the function svd() to compute the SVD of y. This function will return  U ,  V , and the diagonal entries of  D .

```{r}
s <- svd(y)
names(s)
```

You can check that the SVD works by typing:

```{r}
y_svd <- s$u %*% diag(s$d) %*% t(s$v)
max(abs(y - y_svd))
```

Compute the sum of squares of the columns of  Y  and store them in ss_y. Then compute the sum of squares of columns of the transformed  YV  and store them in ss_yv. Confirm that sum(ss_y) is equal to sum(ss_yv).

What is the value of sum(ss_y) (and also the value of sum(ss_yv))?

```{r}
ss_y <- apply((y*y),2, sum)
sum(ss_y)
plot(ss_y)

#we are supposed to find sum of y * YV
# we know YV = UD or U(t)Y or DV(t)

#here is sum of YV
ss_yv <- colSums((y%*%s$v)^2 )
sum(ss_yv)
plot(ss_yv)

#same result with UD
ss_ud <- colSums((s$u %*% diag(s$d))^2)
sum(ss_ud)
plot(ss_ud)
```

#### **Question 4**  
We see that the total sum of squares is preserved. This is because  V  is orthogonal. Now to start understanding how  YV  is useful, plot ss_y against the column number and then do the same for ss_yv.

What do you observe?

- ss_y and ss_yv are decreasing and close to 0 for the 4th column and beyond.
- ss_yv is decreasing and close to 0 for the 4th column and beyond.   [*]
- ss_y is decreasing and close to 0 for the 4th column and beyond.
- There is no discernible pattern to either ss_y or ss_yv.

#### **Question 5**  
Now notice that we didn't have to compute ss_yv because we already have the answer. How? Remember that  YV=UD  and because  U  is orthogonal, we know that the sum of squares of the columns of  UD  are the diagonal entries of  D  squared. Confirm this by plotting the square root of ss_yv versus the diagonal entries of  D .

Which of these plots is correct?

```{r}
#5 plotting UD = diagonal entries of D squared. UD = YV
d_sq <- (s$d)^2
plot(d_sq)

srt_ss_yv <- sqrt(ss_yv)
plot(s$d, srt_ss_yv)
```

#### **Question 6**  
So from the above we know that the sum of squares of the columns of  Y  (the total sum of squares) adds up to the sum of s$d^2 and that the transformation  YV  gives us columns with sums of squares equal to s$d^2. Now compute the percent of the total variability that is explained by just the first three columns of  YV .

What proportion of the total variability is explained by the first three columns of  YV ?

```{r}
variability <- sum(s$d[1:3]^2)/sum(d_sq)
variability
```

#### **Question 7**  
Before we continue, let's show a useful computational trick to avoid creating the matrix diag(s$d). To motivate this, we note that if we write  U  out in its columns  [U1,U2,…,Up]  then  UD  is equal to

UD=[U1d1,1,U2d2,2,…,Updp,p] 
Use the sweep function to compute  UD  without constructing diag(s$d) or using matrix multiplication.

Which code is correct?

```{r}
identical(s$u %*% (diag(s$d)), sweep(s$u, 2, s$d, FUN = "*"))

```

#### **Question 8**  
We know that  U1d1,1 , the first column of  UD , has the most variability of all the columns of  UD . Earlier we looked at an image of  Y  using my_image(y), in which we saw that the student to student variability is quite large and that students that are good in one subject tend to be good in all. This implies that the average (across all subjects) for each student should explain a lot of the variability. Compute the average score for each student, plot it against  U1d1,1 , and describe what you find.

What do you observe?

```{r}
#8 U1d1,1,
u1d11 <- s$u %*% (diag(s$d))
u1d11[,1]
u1d11[,1]

plot(rowMeans(y),u1d11[,1])
plot(u1d11[,1], rowMeans(y))

plot(-s$u[,1]*s$d[1], rowMeans(y))
```

#### **Question 9**  
We note that the signs in SVD are arbitrary because:

UDV⊤=(−U)D(−V)⊤ 
With this in mind we see that the first column of  UD  is almost identical to the average score for each student except for the sign.

This implies that multiplying  Y  by the first column of  V  must be performing a similar operation to taking the average. Make an image plot of  V  and describe the first column relative to others and how this relates to taking an average.

How does the first column relate to the others, and how does this relate to taking an average?

```{r}
my_image(s$v)

(s$v[,1])

dim(s$v)
```

### Comprehension Check: Clustering

These exercises will work with the tissue_gene_expression dataset, which is part of the dslabs package.

#### **Question 1**  
Load the tissue_gene_expression dataset. Remove the row means and compute the distance between each observation. Store the result in d.

Which of the following lines of code correctly does this computation?

```{r}
library(caret)
library(dplyr)
library(dslabs)
library(tidyverse)
data("tissue_gene_expression")
d <- dist(tissue_gene_expression$x - rowMeans(tissue_gene_expression$x))
```

#### **Question 2**  
Make a hierarchical clustering plot and add the tissue types as labels.

You will observe multiple branches.

Which tissue type is in the branch farthest to the left?
```{r}
plot(hclust(d))
```

#### **Question 3**  
Run a k-means clustering on the data with  K=7 . Make a table comparing the identified clusters to the actual tissue types. Run the algorithm several times to see how the answer changes.

What do you observe for the clustering of the liver tissue?

```{r}
clust <- kmeans(tissue_gene_expression$x, centers = 7)
table(clust$cluster, tissue_gene_expression$y)
```

#### **Question 4**  
Select the 50 most variable genes. Make sure the observations show up in the columns, that the predictor are centered, and add a color bar to show the different tissue types. Hint: use the ColSideColors argument to assign colors. Also, use col = RColorBrewer::brewer.pal(11, "RdBu") for a better use of colors.

Part of the code is provided for you here:

```{r}
library(RColorBrewer)
sds <- matrixStats::colSds(tissue_gene_expression$x)
ind <- order(sds, decreasing = TRUE)[1:50]
colors <- brewer.pal(7, "Dark2")[as.numeric(tissue_gene_expression$y)]
```
```{r}
heatmap(t(tissue_gene_expression$x[,ind]), col = brewer.pal(11, "RdBu"), scale = "row", ColSideColors = colors)
```



